{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vinuthna06reddy/VinuthnaReddy_INFO5731_FALL2024/blob/main/INFO5731_Assignment_2_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ryk8D1Q4Wsrp"
      },
      "source": [
        "# **INFO5731 Assignment 2**\n",
        "\n",
        "In this assignment, you will work on gathering text data from an open data source via web scraping or API. Following this, you will need to clean the text data and perform syntactic analysis on the data. Follow the instructions carefully and design well-structured Python programs to address each question.\n",
        "\n",
        "**Expectations**:\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "* **Make sure to submit the cleaned data CSV in the comment section - 10 points**\n",
        "\n",
        "**Total points**: 100\n",
        "\n",
        "**Deadline**: Tuesday, at 11:59 PM.\n",
        "\n",
        "**Late Submission will have a penalty of 10% reduction for each day after the deadline.**\n",
        "\n",
        "**Please check that the link you submitted can be opened and points to the correct assignment.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkzR8cFAyGik"
      },
      "source": [
        "# Question 1 (40 points)\n",
        "\n",
        "Write a python program to collect text data from **either of the following sources** and save the data into a **csv file:**\n",
        "\n",
        "(1) Collect all the customer reviews of a product (you can choose any porduct) on amazon. [atleast 1000 reviews]\n",
        "\n",
        "(2) Collect the top 1000 User Reviews of a movie recently in 2023 or 2024 (you can choose any movie) from IMDB. [If one movie doesn't have sufficient reviews, collect reviews of atleast 2 or 3 movies]\n",
        "\n",
        "(3) Collect all the reviews of the top 1000 most popular software from G2 or Capterra.\n",
        "\n",
        "(4) Collect the **abstracts** of the top 10000 research papers by using the query \"machine learning\", \"data science\", \"artifical intelligence\", or \"information extraction\" from Semantic Scholar.\n",
        "\n",
        "(5) Collect all the information of the 904 narrators in the Densho Digital Repository.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests beautifulsoup4 pandas\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import re\n",
        "import spacy\n",
        "\n",
        "def scrape_imdb_reviews(movie_url, num_reviews=1000):\n",
        "    reviews = []\n",
        "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "\n",
        "    for start in range(0, num_reviews, 25):\n",
        "        url = f\"{movie_url}reviews?ref_=tt_ql_3&start={start}\"\n",
        "        response = requests.get(url, headers=headers)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        review_containers = soup.find_all('div', class_='text show-more__control')\n",
        "        for review in review_containers:\n",
        "            reviews.append(review.text)\n",
        "\n",
        "        if len(reviews) >= num_reviews:\n",
        "            break\n",
        "\n",
        "    return reviews[:num_reviews]\n",
        "movie_url = \"https://www.imdb.com/title/tt9603212/reviews/?ref_=tt_ov_ql_2/\"\n",
        "reviews = scrape_imdb_reviews(movie_url)\n",
        "\n",
        "df = pd.DataFrame(reviews, columns=['review'])\n",
        "df.to_csv('imdb_reviews.csv', index=False)\n",
        "\n",
        "print(\"Reviews collected and saved to 'imdb_reviews.csv'\")\n",
        "\n",
        "def clean_reviews(reviews):\n",
        "    cleaned_reviews = []\n",
        "    for review in reviews:\n",
        "        review = re.sub(r'\\s+', ' ', review)\n",
        "        review = re.sub(r'[^\\w\\s]', '', review)\n",
        "        review = review.lower()\n",
        "        cleaned_reviews.append(review)\n",
        "    return cleaned_reviews\n",
        "\n",
        "df = pd.read_csv('imdb_reviews.csv')\n",
        "df['cleaned_review'] = clean_reviews(df['review'])\n",
        "df.to_csv('cleaned_imdb_reviews.csv', index=False)\n",
        "print(\"Reviews cleaned and saved to 'cleaned_imdb_reviews.csv'\")\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "def syntactic_analysis(reviews):\n",
        "    analyzed_data = []\n",
        "    for review in reviews:\n",
        "        doc = nlp(review)\n",
        "        analyzed_data.append([(token.text, token.pos_, token.dep_) for token in doc])\n",
        "    return analyzed_data\n",
        "df['syntactic_analysis'] = syntactic_analysis(df['cleaned_review'])\n",
        "df.to_csv('analyzed_imdb_reviews.csv', index=False)\n",
        "print(\"Syntactic analysis completed and saved to 'analyzed_imdb_reviews.csv'\")\n"
      ],
      "metadata": {
        "id": "fz-v0U0wnY3I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "472e12de-4c4a-4f07-aaf2-e6e47f7aa94d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.8.30)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Reviews collected and saved to 'imdb_reviews.csv'\n",
            "Reviews cleaned and saved to 'cleaned_imdb_reviews.csv'\n",
            "Syntactic analysis completed and saved to 'analyzed_imdb_reviews.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90_NR8c5XGWc"
      },
      "source": [
        "# Question 2 (30 points)\n",
        "\n",
        "Write a python program to **clean the text data** you collected in the previous question and save the clean data in a new column in the csv file. The data cleaning steps include: [Code and output is required for each part]\n",
        "\n",
        "(1) Remove noise, such as special characters and punctuations.\n",
        "\n",
        "(2) Remove numbers.\n",
        "\n",
        "(3) Remove stopwords by using the stopwords list.\n",
        "\n",
        "(4) Lowercase all texts\n",
        "\n",
        "(5) Stemming.\n",
        "\n",
        "(6) Lemmatization."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "df = pd.read_csv('analyzed_imdb_reviews.csv')\n",
        "\n",
        "def remove_noise(text):\n",
        "    return re.sub(r'[^\\w\\s]', '', text)\n",
        "def remove_numbers(text):\n",
        "    return re.sub(r'\\d+', '', text)\n",
        "stop_words = set(stopwords.words('english'))\n",
        "def remove_stopwords(text):\n",
        "    return ' '.join([word for word in text.split() if word not in stop_words])\n",
        "def lowercase(text):\n",
        "    return text.lower()\n",
        "stemmer = PorterStemmer()\n",
        "def stemming(text):\n",
        "    return ' '.join([stemmer.stem(word) for word in text.split()])\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "def lemmatization(text):\n",
        "    return ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
        "df['cleaned_text'] = df['cleaned_review'].apply(remove_noise)\n",
        "df['cleaned_text'] = df['cleaned_text'].apply(remove_numbers)\n",
        "df['cleaned_text'] = df['cleaned_text'].apply(remove_stopwords)\n",
        "df['cleaned_text'] = df['cleaned_text'].apply(lowercase)\n",
        "df['cleaned_text'] = df['cleaned_text'].apply(stemming)\n",
        "df['cleaned_text'] = df['cleaned_text'].apply(lemmatization)\n",
        "\n",
        "df.to_csv('final_cleaned_imdb_reviews.csv', index=False)\n",
        "\n",
        "print(df[['cleaned_review', 'cleaned_text']].head())\n"
      ],
      "metadata": {
        "id": "xgKBQ-ycnXOe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e735c14-2fcb-4993-f19a-f648de60d887"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                      cleaned_review  \\\n",
            "0  man i wish i loved this movie more than i did ...   \n",
            "1  just got out of seeing this movie i am really ...   \n",
            "2  just saw this one in a theatre and boy what an...   \n",
            "3  to me a new mi film is an event from part 4 on...   \n",
            "4  good god i feel ashamed for approaching this f...   \n",
            "\n",
            "                                        cleaned_text  \n",
            "0  man wish love movi dont get wrong solid action...  \n",
            "1  got see movi realli disappoint total kill pote...  \n",
            "2  saw one theatr boy adrenalin rush latest missi...  \n",
            "3  new mi film event part onward theyv spectacula...  \n",
            "4  good god feel asham approach film modicum doub...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1F_PZdH9Sh49"
      },
      "source": [
        "# Question 3 (30 points)\n",
        "\n",
        "Write a python program to **conduct syntax and structure analysis of the clean text** you just saved above. The syntax and structure analysis includes:\n",
        "\n",
        "(1) **Parts of Speech (POS) Tagging:** Tag Parts of Speech of each word in the text, and calculate the total number of N(oun), V(erb), Adj(ective), Adv(erb), respectively.\n",
        "\n",
        "(2) **Constituency Parsing and Dependency Parsing:** print out the constituency parsing trees and dependency parsing trees of all the sentences. Using one sentence as an example to explain your understanding about the constituency parsing tree and dependency parsing tree.\n",
        "\n",
        "(3) **Named Entity Recognition:** Extract all the entities such as person names, organizations, locations, product names, and date from the clean texts, calculate the count of each entity."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy textblob nltk\n",
        "!python -m spacy download en_core_web_sm\n",
        "import spacy\n",
        "from textblob import TextBlob\n",
        "from collections import Counter\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "cleaned_texts = [\n",
        "    \"man wish love movi dont get wrong solid action\"\n",
        "    \"got see movi realli disappoint total kill potential\"\n",
        "    \"saw one theatr boy adrenalin rush latest missiion impossible\"\n",
        "    \"new mi film event part onward theyv spectacular\"\n",
        "    \"good god feel asham approach film modicum doubt\"\n",
        "]\n",
        "pos_counts = Counter({'Nouns': 0, 'Verbs': 0, 'Adjectives': 0, 'Adverbs': 0})\n",
        "\n",
        "def get_pos_category(tag):\n",
        "    if tag.startswith('N'):\n",
        "        return 'Nouns'\n",
        "    elif tag.startswith('V'):\n",
        "        return 'Verbs'\n",
        "    elif tag.startswith('J'):\n",
        "        return 'Adjectives'\n",
        "    elif tag.startswith('R'):\n",
        "        return 'Adverbs'\n",
        "    return None\n",
        "for text in cleaned_texts:\n",
        "    blob = TextBlob(text)\n",
        "    tags = blob.tags\n",
        "    for word, pos in tags:\n",
        "        category = get_pos_category(pos)\n",
        "        if category:\n",
        "            pos_counts[category] += 1\n",
        "\n",
        "print(\"POS Tagging Counts:\", pos_counts)\n",
        "\n",
        "def analyze_parsing(text):\n",
        "    doc = nlp(text)\n",
        "    print(f\"\\nAnalyzing the text: '{text}'\")\n",
        "    print(\"Constituency Parsing (approximated as dependency parsing):\", doc)\n",
        "    print(\"\\nDependency Parsing:\")\n",
        "    for token in doc:\n",
        "        print(f\"{token.text} --> {token.dep_} ({token.head.text})\")\n",
        "    print(\"\\nDependency Tree:\")\n",
        "    for token in doc:\n",
        "        print(f\"{token.text} ({token.dep_}) <-- {token.head.text} ({token.head.dep_})\")\n",
        "example_text = cleaned_texts[0]\n",
        "analyze_parsing(example_text)\n",
        "ner_counts = Counter()\n",
        "\n",
        "print(\"\\nNamed Entity Recognition Results:\")\n",
        "for text in cleaned_texts:\n",
        "    doc = nlp(text)\n",
        "    for ent in doc.ents:\n",
        "        ner_counts[ent.label_] += 1\n",
        "        print(f\"Entity: {ent.text}, Label: {ent.label_}\")\n",
        "print(\"\\nNER Entity Counts:\", ner_counts)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W5vg8lKc06oe",
        "outputId": "fd54da1b-80d4-4ddf-f96f-e18b433784b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.5)\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.10/dist-packages (0.17.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (71.0.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.8.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.19.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (71.0.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.8.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.19.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POS Tagging Counts: Counter({'Nouns': 24, 'Adjectives': 6, 'Verbs': 5, 'Adverbs': 0})\n",
            "\n",
            "Analyzing the text: 'man wish love movi dont get wrong solid actiongot see movi realli disappoint total kill potentialsaw one theatr boy adrenalin rush latest missiion impossiblenew mi film event part onward theyv spectaculargood god feel asham approach film modicum doubt'\n",
            "Constituency Parsing (approximated as dependency parsing): man wish love movi dont get wrong solid actiongot see movi realli disappoint total kill potentialsaw one theatr boy adrenalin rush latest missiion impossiblenew mi film event part onward theyv spectaculargood god feel asham approach film modicum doubt\n",
            "\n",
            "Dependency Parsing:\n",
            "man --> nsubj (wish)\n",
            "wish --> ROOT (wish)\n",
            "love --> compound (movi)\n",
            "movi --> nsubj (get)\n",
            "do --> aux (get)\n",
            "nt --> neg (get)\n",
            "get --> ccomp (wish)\n",
            "wrong --> amod (actiongot)\n",
            "solid --> amod (actiongot)\n",
            "actiongot --> intj (see)\n",
            "see --> conj (wish)\n",
            "movi --> compound (realli)\n",
            "realli --> compound (disappoint)\n",
            "disappoint --> compound (potentialsaw)\n",
            "total --> amod (potentialsaw)\n",
            "kill --> compound (potentialsaw)\n",
            "potentialsaw --> dobj (see)\n",
            "one --> nummod (theatr)\n",
            "theatr --> compound (boy)\n",
            "boy --> compound (adrenalin)\n",
            "adrenalin --> nsubj (rush)\n",
            "rush --> nsubj (doubt)\n",
            "latest --> amod (part)\n",
            "missiion --> compound (mi)\n",
            "impossiblenew --> compound (mi)\n",
            "mi --> compound (event)\n",
            "film --> compound (event)\n",
            "event --> compound (part)\n",
            "part --> dobj (rush)\n",
            "onward --> advmod (theyv)\n",
            "theyv --> amod (spectaculargood)\n",
            "spectaculargood --> npadvmod (rush)\n",
            "god --> nsubj (feel)\n",
            "feel --> relcl (spectaculargood)\n",
            "asham --> compound (approach)\n",
            "approach --> compound (film)\n",
            "film --> compound (modicum)\n",
            "modicum --> appos (spectaculargood)\n",
            "doubt --> ROOT (doubt)\n",
            "\n",
            "Dependency Tree:\n",
            "man (nsubj) <-- wish (ROOT)\n",
            "wish (ROOT) <-- wish (ROOT)\n",
            "love (compound) <-- movi (nsubj)\n",
            "movi (nsubj) <-- get (ccomp)\n",
            "do (aux) <-- get (ccomp)\n",
            "nt (neg) <-- get (ccomp)\n",
            "get (ccomp) <-- wish (ROOT)\n",
            "wrong (amod) <-- actiongot (intj)\n",
            "solid (amod) <-- actiongot (intj)\n",
            "actiongot (intj) <-- see (conj)\n",
            "see (conj) <-- wish (ROOT)\n",
            "movi (compound) <-- realli (compound)\n",
            "realli (compound) <-- disappoint (compound)\n",
            "disappoint (compound) <-- potentialsaw (dobj)\n",
            "total (amod) <-- potentialsaw (dobj)\n",
            "kill (compound) <-- potentialsaw (dobj)\n",
            "potentialsaw (dobj) <-- see (conj)\n",
            "one (nummod) <-- theatr (compound)\n",
            "theatr (compound) <-- boy (compound)\n",
            "boy (compound) <-- adrenalin (nsubj)\n",
            "adrenalin (nsubj) <-- rush (nsubj)\n",
            "rush (nsubj) <-- doubt (ROOT)\n",
            "latest (amod) <-- part (dobj)\n",
            "missiion (compound) <-- mi (compound)\n",
            "impossiblenew (compound) <-- mi (compound)\n",
            "mi (compound) <-- event (compound)\n",
            "film (compound) <-- event (compound)\n",
            "event (compound) <-- part (dobj)\n",
            "part (dobj) <-- rush (nsubj)\n",
            "onward (advmod) <-- theyv (amod)\n",
            "theyv (amod) <-- spectaculargood (npadvmod)\n",
            "spectaculargood (npadvmod) <-- rush (nsubj)\n",
            "god (nsubj) <-- feel (relcl)\n",
            "feel (relcl) <-- spectaculargood (npadvmod)\n",
            "asham (compound) <-- approach (compound)\n",
            "approach (compound) <-- film (compound)\n",
            "film (compound) <-- modicum (appos)\n",
            "modicum (appos) <-- spectaculargood (npadvmod)\n",
            "doubt (ROOT) <-- doubt (ROOT)\n",
            "\n",
            "Named Entity Recognition Results:\n",
            "Entity: one, Label: CARDINAL\n",
            "Entity: adrenalin, Label: ORG\n",
            "\n",
            "NER Entity Counts: Counter({'CARDINAL': 1, 'ORG': 1})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Comment**\n",
        "Make sure to submit the cleaned data CSV in the comment section - 10 points"
      ],
      "metadata": {
        "id": "CXNn1lEVbMsv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://myunt-my.sharepoint.com/personal/vinuthnavintha_my_unt_edu/Documents/final_cleaned_imdb_reviews.csv?web=1"
      ],
      "metadata": {
        "id": "i7if6lue5wC9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question\n",
        "\n",
        "Provide your thoughts on the assignment. What did you find challenging, and what aspects did you enjoy? Your opinion on the provided time to complete the assignment."
      ],
      "metadata": {
        "id": "q8BFCvWp32cf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "This assignment was a well-rounded introduction to web scraping, text data cleaning, and syntactic analysis, which are important skills in natural language processing (NLP).\n",
        "Though, this assignment took more time as it required learning to scrape websites, handle dynamic content, clean unstructured text, and perform NLP tasks using libraries like spacy."
      ],
      "metadata": {
        "id": "Qh3qMJ_KoyZD"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "OP4-5qxAtInv"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vinuthna06reddy/VinuthnaReddy_INFO5731_FALL2024/blob/main/INFO5731_Exercise_3_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdRwkJBn70nX"
      },
      "source": [
        "# **INFO5731 In-class Exercise 3**\n",
        "\n",
        "The purpose of this exercise is to explore various aspects of text analysis, including feature extraction, feature selection, and text similarity ranking.\n",
        "\n",
        "**Expectations**:\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of Friday, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission. , and no requests will be answered. Manage your time accordingly.**\n",
        "\n",
        "**Please check that the link you submitted can be opened and points to the correct assignment.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARqm7u6B70ne"
      },
      "source": [
        "## Question 1 (10 Points)\n",
        "Describe an interesting **text classification or text mining task** and explain what kind of features might be useful for you to build the machine learning model. List your features and explain why these features might be helpful. You need to list at least five different types of features. **Your dataset must be text.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VAZj4PHB70nf"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Customer Intent Classification:\n",
        "It is a task in Natural Language Processing(NLP) where the goal is to purpose of intent behind a customer's input. Typically in the form of text such as chat messages, reviews, or emails.\n",
        "\n",
        "Useful features for building a Machine Learning Model:\n",
        "As the dataset is text-based, the features extracted from the text will help in identifying the customer’s intent.\n",
        "\n",
        "1.Bag of Words (BoW) / Term Frequency-Inverse Document Frequency (TF-IDF)\n",
        "    BoW captures word frequencies in a document, while TF-IDF adjusts those frequencies based on how common the words are across documents.\n",
        "  Ex: Common words like “buy,” “help,” or “issue” can directly hint at specific intents. For instance, words like \"buy\" or \"order\" may indicate a \"Purchase Intent,\" while words like \"issue\" or \"problem\" may indicate \"Support Request.\"\n",
        "\n",
        "2.N-grams (Bigrams, Trigrams)\n",
        "    N-grams are sequences of words. Bigrams (two-word sequences) and trigrams (three-word sequences) are useful for capturing context in phrases.\n",
        "  Ex: Phrases like \"how to buy\" or \"need support\" provide more context than individual words. These word combinations are often more predictive of intent than single words.\n",
        "\n",
        "3.Sentiment Analysis\n",
        "    Sentiment analysis detects the emotional tone of the text (positive, negative, or neutral).\n",
        "  Ex:Customer inquiries with negative sentiment might indicate frustration or a need for support, whereas positive sentiment might indicate feedback or praise. Sentiment can help distinguish between \"Request for Support\" and \"Feedback.\"\n",
        "\n",
        "4.Word Embeddings\n",
        "    Word embeddings capture the semantic relationships between words by representing them as dense vectors in a high-dimensional space.\n",
        "  They help capture the meaning of words in context. Words like “purchase” and “order” may be used interchangeably in different contexts, and embeddings would capture that similarity.\n",
        "\n",
        "5.Length of the Text\n",
        "    The number of words or characters in the input text.\n",
        "  Shorter texts may indicate quick inquiries or commands, while longer texts may indicate detailed feedback or complex support requests.\n",
        "\n",
        "6.Part of Speech (POS) Tags\n",
        "    This feature represents the grammatical structure of the sentence by tagging each word with its part of speech (e.g., noun, verb, adjective).\n",
        "  Ex: Different types of intent might be associated with different grammatical structures. For example, \"Request\" intents may frequently include modal verbs (\"can,\" \"would\"), while \"Feedback\" might include more adjectives (\"great,\" \"bad\").\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEUjBE6C70nf"
      },
      "source": [
        "## Question 2 (10 Points)\n",
        "Write python code to extract these features you discussed above. You can collect a few sample text data for the feature extraction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EoQX5s4O70nf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff81a5d1-adac-4604-be55-46d4ecc0b0b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bag of Words (BoW) features:\n",
            " [[0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0]\n",
            " [1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 0]\n",
            " [0 0 0 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0]\n",
            " [0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0]\n",
            " [0 0 0 1 0 0 0 1 0 0 0 1 0 0 1 0 0 0 0 1 1 1 1 0 0 1]]\n",
            "BoW feature names: ['account' 'billing' 'buy' 'can' 'great' 'having' 'help' 'how' 'is'\n",
            " 'issues' 'it' 'item' 'laptop' 'love' 'me' 'my' 'need' 'new' 'product'\n",
            " 'return' 'tell' 'the' 'to' 'want' 'with' 'you']\n",
            "\n",
            "TF-IDF features:\n",
            " [[0.         0.         0.46369322 0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.46369322 0.         0.         0.         0.         0.46369322\n",
            "  0.         0.         0.         0.         0.37410477 0.46369322\n",
            "  0.         0.        ]\n",
            " [0.46369322 0.         0.         0.         0.         0.\n",
            "  0.46369322 0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.46369322 0.46369322 0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.37410477 0.        ]\n",
            " [0.         0.         0.         0.         0.42841136 0.\n",
            "  0.         0.         0.42841136 0.         0.42841136 0.\n",
            "  0.         0.42841136 0.         0.         0.         0.\n",
            "  0.42841136 0.         0.         0.28691208 0.         0.\n",
            "  0.         0.        ]\n",
            " [0.         0.49389914 0.         0.         0.         0.49389914\n",
            "  0.         0.         0.         0.49389914 0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.33077001 0.         0.\n",
            "  0.39847472 0.        ]\n",
            " [0.         0.         0.         0.35137655 0.         0.\n",
            "  0.         0.35137655 0.         0.         0.         0.35137655\n",
            "  0.         0.         0.35137655 0.         0.         0.\n",
            "  0.         0.35137655 0.35137655 0.23532097 0.28348839 0.\n",
            "  0.         0.35137655]]\n",
            "TF-IDF feature names: ['account' 'billing' 'buy' 'can' 'great' 'having' 'help' 'how' 'is'\n",
            " 'issues' 'it' 'item' 'laptop' 'love' 'me' 'my' 'need' 'new' 'product'\n",
            " 'return' 'tell' 'the' 'to' 'want' 'with' 'you']\n",
            "\n",
            "N-gram features (Bigrams/Trigrams):\n",
            " [[1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0\n",
            "  0 1 1 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 1 1 0 0 0 0]\n",
            " [0 0 0 0 1 1 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 1 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
            "  0 0 0 0 0 1 1 0 0]\n",
            " [0 0 1 1 0 0 0 0 0 0 1 1 0 0 0 0 0 1 1 0 0 0 0 0 0 1 1 1 1 0 1 0 0 0 0 1\n",
            "  1 0 0 0 0 0 0 1 1]]\n",
            "N-gram feature names: ['buy new' 'buy new laptop' 'can you' 'can you tell' 'great love'\n",
            " 'great love it' 'having issues' 'having issues with' 'help with'\n",
            " 'help with my' 'how to' 'how to return' 'is great' 'is great love'\n",
            " 'issues with' 'issues with the' 'love it' 'me how' 'me how to'\n",
            " 'my account' 'need help' 'need help with' 'new laptop' 'product is'\n",
            " 'product is great' 'return the' 'return the item' 'tell me' 'tell me how'\n",
            " 'the billing' 'the item' 'the product' 'the product is' 'to buy'\n",
            " 'to buy new' 'to return' 'to return the' 'want to' 'want to buy'\n",
            " 'with my' 'with my account' 'with the' 'with the billing' 'you tell'\n",
            " 'you tell me']\n",
            "\n",
            "Sentiment Scores:\n",
            "I want to buy a new laptop. --> {'neg': 0.0, 'neu': 0.755, 'pos': 0.245, 'compound': 0.0772}\n",
            "I need help with my account. --> {'neg': 0.0, 'neu': 0.597, 'pos': 0.403, 'compound': 0.4019}\n",
            "The product is great, I love it! --> {'neg': 0.0, 'neu': 0.318, 'pos': 0.682, 'compound': 0.8622}\n",
            "I'm having issues with the billing. --> {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
            "Can you tell me how to return the item? --> {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
            "\n",
            "Word Embeddings (SpaCy):\n",
            "Text: I want to buy a new laptop.\n",
            "Embedding shape: (96,)\n",
            "First 5 elements of embedding: [-0.43398985 -0.66643625 -0.06013297 -0.14153533 -0.30066517]\n",
            "\n",
            "Text: I need help with my account.\n",
            "Embedding shape: (96,)\n",
            "First 5 elements of embedding: [-0.42343855 -0.16432913 -0.00281935  0.00809826 -0.18513033]\n",
            "\n",
            "Text: The product is great, I love it!\n",
            "Embedding shape: (96,)\n",
            "First 5 elements of embedding: [-0.0027199   0.02604449  0.05820492 -0.00780148 -0.15428787]\n",
            "\n",
            "Text: I'm having issues with the billing.\n",
            "Embedding shape: (96,)\n",
            "First 5 elements of embedding: [-0.2747683   0.24372312 -0.08253606 -0.1782432   0.08605531]\n",
            "\n",
            "Text: Can you tell me how to return the item?\n",
            "Embedding shape: (96,)\n",
            "First 5 elements of embedding: [-0.5518259  -0.43271288  0.16489454  0.14512467 -0.2924174 ]\n",
            "\n",
            "\n",
            "Length of Text (in words):\n",
            "I want to buy a new laptop. --> 8 words\n",
            "I need help with my account. --> 7 words\n",
            "The product is great, I love it! --> 9 words\n",
            "I'm having issues with the billing. --> 8 words\n",
            "Can you tell me how to return the item? --> 10 words\n",
            "\n",
            "Part of Speech (POS) Tags:\n",
            "\n",
            "Text: I want to buy a new laptop.\n",
            "I (PRON) want (VERB) to (PART) buy (VERB) a (DET) new (ADJ) laptop (NOUN) . (PUNCT) \n",
            "\n",
            "Text: I need help with my account.\n",
            "I (PRON) need (VERB) help (NOUN) with (ADP) my (PRON) account (NOUN) . (PUNCT) \n",
            "\n",
            "Text: The product is great, I love it!\n",
            "The (DET) product (NOUN) is (AUX) great (ADJ) , (PUNCT) I (PRON) love (VERB) it (PRON) ! (PUNCT) \n",
            "\n",
            "Text: I'm having issues with the billing.\n",
            "I (PRON) 'm (AUX) having (VERB) issues (NOUN) with (ADP) the (DET) billing (NOUN) . (PUNCT) \n",
            "\n",
            "Text: Can you tell me how to return the item?\n",
            "Can (AUX) you (PRON) tell (VERB) me (PRON) how (SCONJ) to (PART) return (VERB) the (DET) item (NOUN) ? (PUNCT) \n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import spacy\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "import numpy as np\n",
        "\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "sample_texts = [\n",
        "    \"I want to buy a new laptop.\",\n",
        "    \"I need help with my account.\",\n",
        "    \"The product is great, I love it!\",\n",
        "    \"I'm having issues with the billing.\",\n",
        "    \"Can you tell me how to return the item?\"\n",
        "]\n",
        "\n",
        "bow_vectorizer = CountVectorizer()\n",
        "bow_features = bow_vectorizer.fit_transform(sample_texts)\n",
        "\n",
        "print(\"Bag of Words (BoW) features:\\n\", bow_features.toarray())\n",
        "print(\"BoW feature names:\", bow_vectorizer.get_feature_names_out())\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_features = tfidf_vectorizer.fit_transform(sample_texts)\n",
        "\n",
        "print(\"\\nTF-IDF features:\\n\", tfidf_features.toarray())\n",
        "print(\"TF-IDF feature names:\", tfidf_vectorizer.get_feature_names_out())\n",
        "\n",
        "ngram_vectorizer = CountVectorizer(ngram_range=(2, 3))\n",
        "ngram_features = ngram_vectorizer.fit_transform(sample_texts)\n",
        "\n",
        "print(\"\\nN-gram features (Bigrams/Trigrams):\\n\", ngram_features.toarray())\n",
        "print(\"N-gram feature names:\", ngram_vectorizer.get_feature_names_out())\n",
        "\n",
        "sentiment_analyzer = SentimentIntensityAnalyzer()\n",
        "sentiment_scores = [sentiment_analyzer.polarity_scores(text) for text in sample_texts]\n",
        "\n",
        "print(\"\\nSentiment Scores:\")\n",
        "for text, score in zip(sample_texts, sentiment_scores):\n",
        "    print(f\"{text} --> {score}\")\n",
        "\n",
        "print(\"\\nWord Embeddings (SpaCy):\")\n",
        "for text in sample_texts:\n",
        "    doc = nlp(text)\n",
        "    print(f\"Text: {text}\")\n",
        "    print(f\"Embedding shape: {doc.vector.shape}\")\n",
        "    print(f\"First 5 elements of embedding: {doc.vector[:5]}\\n\")\n",
        "\n",
        "text_lengths = [len(nltk.word_tokenize(text)) for text in sample_texts]\n",
        "\n",
        "print(\"\\nLength of Text (in words):\")\n",
        "for text, length in zip(sample_texts, text_lengths):\n",
        "    print(f\"{text} --> {length} words\")\n",
        "\n",
        "print(\"\\nPart of Speech (POS) Tags:\")\n",
        "for text in sample_texts:\n",
        "    doc = nlp(text)\n",
        "    print(f\"\\nText: {text}\")\n",
        "    for token in doc:\n",
        "        print(f\"{token.text} ({token.pos_})\", end=' ')\n",
        "    print()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oSK4soH70nf"
      },
      "source": [
        "## Question 3 (10 points):\n",
        "Use any of the feature selection methods mentioned in this paper \"Deng, X., Li, Y., Weng, J., & Zhang, J. (2019). Feature selection for text classification: A review. Multimedia Tools & Applications, 78(3).\"\n",
        "\n",
        "Select the most important features you extracted above, rank the features based on their importance in the descending order."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.feature_selection import chi2\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "texts = [\n",
        "    \"I want to buy a new laptop.\",\n",
        "    \"I need help with my account.\"\n",
        "]\n",
        "labels = [\"Purchase Intent\", \"Support Request\"]\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(texts)\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(labels)\n",
        "\n",
        "chi2_values, p_values = chi2(X, y)\n",
        "\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "chi2_df = pd.DataFrame({'Feature': feature_names, 'Chi2': chi2_values})\n",
        "chi2_df = chi2_df.sort_values(by='Chi2', ascending=False).reset_index(drop=True)\n",
        "\n",
        "print(chi2_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1hCbx17puU2k",
        "outputId": "3ced869c-531e-4b57-c883-f23a84ae4e9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Feature      Chi2\n",
            "0  account  0.447214\n",
            "1      buy  0.447214\n",
            "2     help  0.447214\n",
            "3   laptop  0.447214\n",
            "4       my  0.447214\n",
            "5     need  0.447214\n",
            "6      new  0.447214\n",
            "7       to  0.447214\n",
            "8     want  0.447214\n",
            "9     with  0.447214\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nZGAOwl70ng"
      },
      "source": [
        "## Question 4 (10 points):\n",
        "Write python code to rank the text based on text similarity. Based on the text data you used for question 2, design a query to match the most relevant docments. Please use the BERT model to represent both your query and the text data, then calculate the cosine similarity between the query and each text in your data. Rank the similary with descending order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4HoWK-i70ng",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d716e15a-8025-4396-8c60-e64d8654b3cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: Can you please help me return this?.\n",
            "\n",
            "Ranked Texts based on Similarity:\n",
            "Text: Can you tell me how to return the item? -- Similarity Score: 0.8308\n",
            "Text: I need help with my account. -- Similarity Score: 0.6154\n",
            "Text: I want to buy a new laptop. -- Similarity Score: 0.5814\n",
            "Text: I'm having issues with the billing. -- Similarity Score: 0.5672\n",
            "Text: The product is great, I love it! -- Similarity Score: 0.5554\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers torch scikit-learn\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "sample_texts = [\n",
        "    \"I want to buy a new laptop.\",\n",
        "    \"I need help with my account.\",\n",
        "    \"The product is great, I love it!\",\n",
        "    \"I'm having issues with the billing.\",\n",
        "    \"Can you tell me how to return the item?\"\n",
        "]\n",
        "\n",
        "def get_bert_embedding(text, model, tokenizer):\n",
        "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    embeddings = outputs.last_hidden_state.mean(dim=1)\n",
        "    return embeddings\n",
        "\n",
        "text_embeddings = []\n",
        "for text in sample_texts:\n",
        "    text_embedding = get_bert_embedding(text, model, tokenizer)\n",
        "    text_embeddings.append(text_embedding)\n",
        "\n",
        "text_embeddings = torch.cat(text_embeddings)\n",
        "\n",
        "query = \"Can you please help me return this?.\"\n",
        "\n",
        "query_embedding = get_bert_embedding(query, model, tokenizer)\n",
        "\n",
        "cosine_similarities = cosine_similarity(query_embedding, text_embeddings)[0]\n",
        "\n",
        "ranked_indices = np.argsort(-cosine_similarities)\n",
        "\n",
        "print(f\"Query: {query}\\n\")\n",
        "print(\"Ranked Texts based on Similarity:\")\n",
        "for idx in ranked_indices:\n",
        "    print(f\"Text: {sample_texts[idx]} -- Similarity Score: {cosine_similarities[idx]:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question"
      ],
      "metadata": {
        "id": "VEs-OoDEhTW4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important: Reflective Feedback on this exercise**\n",
        "\n",
        "Please provide your thoughts and feedback on the exercises you completed in this assignment. Consider the following points in your response:\n",
        "\n",
        "Learning Experience: Describe your overall learning experience in working on extracting features from text data. What were the key concepts or techniques you found most beneficial in understanding the process?\n",
        "\n",
        "Challenges Encountered: Were there specific difficulties in completing this exercise?\n",
        "\n",
        "Relevance to Your Field of Study: How does this exercise relate to the field of NLP?\n",
        "\n",
        "**(Your submission will not be graded if this question is left unanswered)**\n",
        "\n"
      ],
      "metadata": {
        "id": "IUKC7suYhVl0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "Learning experience:\n",
        "Working on extracting features from text data has been a valuable experience, particularly in understanding how raw textual information can be transformed into meaningful numerical representations for various NLP tasks.\n",
        "The most beneficial concepts which I felt interesting are: Bag of words(BoW) and TF-IDF, N-grams, sentiment analysis, BERT embeddings.\n",
        "\n",
        "Challenges encountered:\n",
        "With the same text dataset, implementing feature extraction using multiple techniques (BoW, TF-IDF, sentiment analysis, embeddings, etc.) needed careful execution and planning.  It needed time and careful consideration to make sure that the various approaches could be seamlessly included into the pipeline.\n",
        "\n",
        "Relevance to the field of study:\n",
        "This practice is extremely useful in Natural Language Processing (NLP), especially for problems involving text similarity, sentiment analysis, and intent categorization. In almost all NLP tasks, the basic technique of transforming unprocessed text into numerical attributes is essential.\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "CAq0DZWAhU9m"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}